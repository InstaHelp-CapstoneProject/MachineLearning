{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification Emergency Case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-02 22:52:27.483111: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-12-02 22:52:27.502582: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1733154747.518818   16575 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1733154747.523971   16575 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-12-02 22:52:27.544375: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import os\n",
    "import re\n",
    "import string\n",
    "from tqdm import tqdm\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                Text Label\n",
      "0  ada kecelakaan besar di jalan tol banyak korba...  High\n",
      "1  orang ini pingsan dan tidak bernapas segera ki...  High\n",
      "2           ada anak kecil tenggelam dia tidak sadar  High\n",
      "3  kecelakaan sepeda motor dengan luka parah di k...  High\n",
      "4  seorang pria terjatuh dari gedung tinggi seger...  High\n"
     ]
    }
   ],
   "source": [
    "# Load Dataset\n",
    "data = pd.read_csv('Dataset/Clean_emergency_case.csv')\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Normalisasi Teks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                Text Label\n",
      "0  ada kecelakaan besar di jalan tol banyak korba...  High\n",
      "1  orang ini pingsan dan tidak bernapas segera ki...  High\n",
      "2           ada anak kecil tenggelam dia tidak sadar  High\n",
      "3  kecelakaan sepeda motor dengan luka parah di k...  High\n",
      "4  seorang pria terjatuh dari gedung tinggi seger...  High\n"
     ]
    }
   ],
   "source": [
    "# Mengubah teks menjadi huruf kecil\n",
    "data['Text'] = data['Text'].str.lower()\n",
    "\n",
    "# Menghapus tanda baca dan karakter non-alfabet\n",
    "data['Text'] = data['Text'].apply(lambda x: re.sub(r'[^\\w\\s]', '', x))\n",
    "\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                Text  \\\n",
      "0  ada kecelakaan besar di jalan tol banyak korba...   \n",
      "1  orang ini pingsan dan tidak bernapas segera ki...   \n",
      "2           ada anak kecil tenggelam dia tidak sadar   \n",
      "3  kecelakaan sepeda motor dengan luka parah di k...   \n",
      "4  seorang pria terjatuh dari gedung tinggi seger...   \n",
      "\n",
      "                                              tokens  \n",
      "0  [ada, kecelakaan, besar, di, jalan, tol, banya...  \n",
      "1  [orang, ini, pingsan, dan, tidak, bernapas, se...  \n",
      "2   [ada, anak, kecil, tenggelam, dia, tidak, sadar]  \n",
      "3  [kecelakaan, sepeda, motor, dengan, luka, para...  \n",
      "4  [seorang, pria, terjatuh, dari, gedung, tinggi...  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to /home/rafli/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('punkt_tab')\n",
    "\n",
    "# Tokenization teks\n",
    "data['tokens'] = data['Text'].apply(word_tokenize)\n",
    "\n",
    "print(data[['Text', 'tokens']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Stop Words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/rafli/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                Text  \\\n",
      "0  ada kecelakaan besar di jalan tol banyak korba...   \n",
      "1  orang ini pingsan dan tidak bernapas segera ki...   \n",
      "2           ada anak kecil tenggelam dia tidak sadar   \n",
      "3  kecelakaan sepeda motor dengan luka parah di k...   \n",
      "4  seorang pria terjatuh dari gedung tinggi seger...   \n",
      "\n",
      "                                              tokens  \n",
      "0   [kecelakaan, besar, jalan, tol, korban, terluka]  \n",
      "1  [orang, pingsan, tidak, bernapas, kirim, ambul...  \n",
      "2             [anak, kecil, tenggelam, tidak, sadar]  \n",
      "3   [kecelakaan, sepeda, motor, luka, parah, kepala]  \n",
      "4           [pria, terjatuh, gedung, kirim, bantuan]  \n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "\n",
    "# Mengambil stopwords Bahasa Indonesia\n",
    "stop_words = set(stopwords.words('indonesian'))\n",
    "\n",
    "# Menghapus kata-kata tertentu dari daftar stopwords\n",
    "custom_stopwords = {\"tidak\", \"kecil\", \"besar\"}  # Kata-kata yang ingin dipertahankan\n",
    "stop_words = stop_words - custom_stopwords  # Menghapus kata-kata tersebut dari stopwords\n",
    "\n",
    "# Menghapus stopwords\n",
    "data['tokens'] = data['tokens'].apply(lambda tokens: [word for word in tokens if word not in stop_words])\n",
    "\n",
    "print(data[['Text', 'tokens']].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "\n",
    "# Menginisialisasi stemmer Bahasa Indonesia\n",
    "factory = StemmerFactory()\n",
    "stemmer = factory.create_stemmer()\n",
    "\n",
    "# Melakukan stemming pada token\n",
    "data['tokens'] = data['tokens'].apply(lambda tokens: [stemmer.stem(word) for word in tokens])\n",
    "\n",
    "# Menampilkan hasil\n",
    "print(data[['Text', 'tokens']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Membuat tokenizer\n",
    "tokenizer = Tokenizer()\n",
    "\n",
    "# Melatih tokenizer pada kolom 'tokens'\n",
    "tokenizer.fit_on_texts(data['tokens'])\n",
    "\n",
    "# Mengonversi token menjadi urutan indeks\n",
    "X = tokenizer.texts_to_sequences(data['tokens'])\n",
    "\n",
    "# Melakukan padding untuk membuat panjang urutan seragam\n",
    "X_padded = pad_sequences(X, padding='post', maxlen=100)\n",
    "\n",
    "print(X_padded.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Inisialisasi LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Mengonversi label menjadi angka\n",
    "y = label_encoder.fit_transform(data['Label'])\n",
    "\n",
    "print(y[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(data['tokens'].apply(lambda tokens: ' '.join(tokens)))\n",
    "\n",
    "print(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Membagi data menjadi 80% training dan 20% testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_padded, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(X_train.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary_size = len(tokenizer.word_index) + 1\n",
    "embedding_dim = 100\n",
    "max_sequence_length = 10\n",
    "\n",
    "X_train = tokenizer.texts_to_sequences(data['tokens'])\n",
    "X_train_padded = pad_sequences(X_train, maxlen=max_sequence_length, padding='post', truncating='post')\n",
    "X_test_padded = pad_sequences(X_test, maxlen=max_sequence_length, padding='post', truncating='post')\n",
    "\n",
    "print(\"Vocabulary Size:\", vocabulary_size)\n",
    "print(\"Sequence Length:\", max_sequence_length)\n",
    "print(\"X_train (padded sequences):\", X_train_padded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, Embedding, Conv1D, Bidirectional, GRU, Dense, Dropout, LayerNormalization\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "# Dimensi input untuk teks: panjang urutan (None) dan dimensi vektor kata (misalnya 300 untuk word2vec)\n",
    "input_text = Input(shape=(None,), name=\"input_text\")\n",
    "\n",
    "# Embedding Layer untuk merepresentasikan kata-kata dalam bentuk vektor (misalnya pre-trained embedding atau trainable embeddings)\n",
    "embedding_dim = 300  # Misalnya menggunakan 300 dimensi untuk kata\n",
    "embedding = Embedding(input_dim=5000, output_dim=embedding_dim, input_length=None)(input_text)  # 5000 adalah ukuran vocabulary\n",
    "\n",
    "# Normalisasi input embedding\n",
    "x = LayerNormalization()(embedding)\n",
    "\n",
    "# Convolutional Layer pertama: 32 filter dengan ukuran kernel (3,), padding 'same' untuk mempertahankan dimensi input, dan fungsi aktivasi ReLU\n",
    "x = Conv1D(32, kernel_size=3, padding='same', activation='relu')(x)\n",
    "\n",
    "# Layer normalisasi untuk output dari Conv1D\n",
    "x = LayerNormalization()(x)\n",
    "\n",
    "# Bidirectional GRU Layer pertama dengan 128 unit, menghasilkan output berurutan (return_sequences=True)\n",
    "x = Bidirectional(GRU(128, return_sequences=True))(x)\n",
    "\n",
    "# Bidirectional GRU Layer kedua dengan 128 unit, menghasilkan output berurutan (return_sequences=True)\n",
    "x = Bidirectional(GRU(128, return_sequences=True))(x)\n",
    "\n",
    "# Bidirectional GRU Layer ketiga dengan 128 unit, menghasilkan output berurutan (return_sequences=True)\n",
    "x = Bidirectional(GRU(128, return_sequences=True))(x)\n",
    "\n",
    "# Dropout untuk menghindari overfitting\n",
    "x = Dropout(0.5)(x)\n",
    "\n",
    "# Dense layer untuk output klasifikasi teks (misalnya untuk klasifikasi multi-kelas)\n",
    "output = Dense(10, activation=\"softmax\")(x)  # 10 adalah jumlah kelas untuk klasifikasi\n",
    "\n",
    "# Membuat model dengan input teks dan output hasil prediksi\n",
    "model = Model(input_text, output, name=\"Text_Classification\")\n",
    "\n",
    "# Menampilkan ringkasan arsitektur model\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_padded = np.array(X_train_padded)  # Mengonversi ke array NumPy\n",
    "y_train = np.array(y_train)  # Mengonversi ke array NumPy\n",
    "\n",
    "X_test_padded = np.array(X_test_padded)  # Mengonversi ke array NumPy\n",
    "y_test = np.array(y_test)  # Mengonversi ke array NumPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer='adam', \n",
    "    loss='sparse_categorical_crossentropy', \n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(\n",
    "    X_train, \n",
    "    y_train, \n",
    "    epochs=5, \n",
    "    batch_size=32, \n",
    "    validation_split=(X_test_padded, y_test)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
